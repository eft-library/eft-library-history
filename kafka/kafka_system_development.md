# ğŸ“‚ ëª©ë¡

- ğŸ› ï¸ [Kafka í™˜ê²½ êµ¬ì¶•](./kafka_system_development.md)
- ğŸš€ [ClickHouseì™€ PostgreSQL ë¹„êµ ë° í…Œì´ë¸” ì„¤ê³„](./clickhouse_postgresql.md)
- ğŸ“Š [ëŒ€ì‹œë³´ë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸](./dashboard.md)

# ğŸ› ï¸ Kafka í™˜ê²½ êµ¬ì¶•

í˜„ì¬ ì‚¬ì´íŠ¸ì˜ **ì‚¬ìš©ì í–‰ë™ ë°ì´í„°ëŠ” Google Analyticsì— ì˜ì¡´**í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.  
í•˜ì§€ë§Œ ë„êµ¬ ìì²´ì— ìµìˆ™í•˜ì§€ ì•Šê³ , ê´€ë¦¬ì ì ‘ì† ì •ë³´ê¹Œì§€ í¬í•¨ë˜ëŠ” ë“± **ë¶ˆí¸í•œ ì ì´ ìˆì—ˆìŠµë‹ˆë‹¤.**

ë˜í•œ **ë” ì •ë°€í•œ ì‚¬ìš©ì í–‰ë™ ë¶„ì„ì„ ìœ„í•´ ë³„ë„ì˜ ìˆ˜ì§‘ ì‹œìŠ¤í…œì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨**í–ˆìŠµë‹ˆë‹¤.  
ì´ì— ë”°ë¼, **Kafkaë¥¼ ë„ì…í•˜ì—¬ ì‚¬ìš©ì í˜ì´ì§€ ì´ë™ íˆìŠ¤í† ë¦¬ë¥¼ ì§ì ‘ ìˆ˜ì§‘í•˜ëŠ” ë°©ì‹ì„ ì±„íƒ**í•˜ì˜€ìŠµë‹ˆë‹¤.

Kafkaë¥¼ ì„ íƒí•œ ì£¼ìš” ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ì— ì í•©í•œ êµ¬ì¡°**ë¥¼ ê°€ì§€ê³  ìˆì–´ ì‚¬ìš©ì ì´ë²¤íŠ¸ ìˆ˜ì§‘ì— ë§¤ìš° ì í•©í•©ë‹ˆë‹¤.
- ì¶”í›„ **ì»¤ë®¤ë‹ˆí‹° ê¸°ëŠ¥ì„ ë‹¤ì‹œ í™œì„±í™”**í•˜ê²Œ ë  ê²½ìš°,
  - **ëŒ“ê¸€ ì•Œë¦¼**, **ì¢‹ì•„ìš” ì•Œë¦¼** ë“±ì˜ ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬ì—ë„ Kafkaë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ í™•ì¥ì„± ë©´ì—ì„œë„ ìœ ë¦¬í•˜ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.
- ë‹¨ìˆœí•œ ë¡œê·¸ ìˆ˜ì§‘ì„ ë„˜ì–´ **ì„œë¹„ìŠ¤ ì „ë°˜ì— ì´ë²¤íŠ¸ ê¸°ë°˜ êµ¬ì¡°(event-driven architecture)**ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì„ ë§ˆë ¨í•˜ê³  ì‹¶ì—ˆìŠµë‹ˆë‹¤.

ì•ìœ¼ë¡œ Kafkaë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‚¬ìš©ì í–‰ë™ ë¶„ì„ê³¼ ë”ë¶ˆì–´, ì´ë²¤íŠ¸ ê¸°ë°˜ ì•Œë¦¼ ì‹œìŠ¤í…œê¹Œì§€ ë²”ìœ„ë¥¼ í™•ì¥í•  ê³„íšì…ë‹ˆë‹¤.

## ğŸ§± ì„¤ê³„ ë° ì €ì¥ì†Œ ê²°ì •

ì´ˆê¸°ì—ëŠ” **PostgreSQLì—ë§Œ ì ì¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìƒê°**í–ˆì§€ë§Œ,

ëŒ€ì‹œë³´ë“œë‚˜ í†µê³„ ì²˜ë¦¬ ëª©ì ì„ ê³ ë ¤í•˜ë©´ **OLAP ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ClickHouseê°€ ë” ì í•©í•  ìˆ˜ ìˆë‹¤ëŠ” íŒë‹¨**ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤.

ì²˜ìŒ ë„ì…í•˜ëŠ” ë§Œí¼, ë‘ ì‹œìŠ¤í…œ ëª¨ë‘ì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ì—¬ **ì„±ëŠ¥ ë° í™œìš©ì„± ë¹„êµë„ í•¨ê»˜ ì§„í–‰**í•´ë³´ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.

## âš™ï¸ êµ¬ì¶• ê°œìš”

**PostgreSQLì€ ê¸°ì¡´ì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ” ìƒíƒœì…ë‹ˆë‹¤.**

**Kafkaì™€ ClickHouseë¥¼ ë‹¨ì¼ ì„œë²„(Stand-alone) í™˜ê²½ì—ì„œ êµ¬ì¶•í•©ë‹ˆë‹¤. (ë¦¬ì†ŒìŠ¤ê°€ ë¶€ì¡±í•´ ë¶„ì‚° í™˜ê²½ì€ ê³ ë ¤í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.)**

## ğŸ§© Ubuntu ì»¤ë„ ë§¤ê°œë³€ìˆ˜ ì„¤ì • ì´ìœ 

    Kafkaì™€ ClickHouseë¥¼ ìš´ì˜í•˜ê¸° ì „, ë‹¤ìŒê³¼ ê°™ì€ ì»¤ë„ ì„¤ì •ì´ í•„ìš”í•œ ì´ìœ ë¥¼ ì´í•´í•˜ê³  ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤
    Delay Accountingì€ ë¦¬ëˆ…ìŠ¤ ì»¤ë„ì—ì„œ í”„ë¡œì„¸ìŠ¤ê°€ I/O, ìŠ¤ì¼€ì¤„ë§ ì§€ì—°, ë½ ëŒ€ê¸° ë“±ìœ¼ë¡œ ì¸í•´ ì‹¤ì œë¡œ ìˆ˜í–‰ë˜ì§€ ëª»í•œ ì‹œê°„ì„ ì¸¡ì •í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.
    ClickHouseì—ì„œëŠ” ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ë¶€ í†µê³„ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤
    OSIOWaitMicroseconds : ClickHouseê°€ I/O ëŒ€ê¸° ì‹œê°„ê³¼ ê´€ë ¨ëœ ì§€ì—°ì„ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.
    í•´ë‹¹ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ëœ ê²½ìš°, ì •í™•í•œ ì„±ëŠ¥ ì§„ë‹¨ ë° ì§€ì—° í†µê³„ ìˆ˜ì§‘ì´ ë¶ˆê°€ëŠ¥í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**êµ¬ì¶• ê³¼ì •**

```shell
# java 21 ì„¤ì¹˜
apt install openjdk-21-jre-headless

# kafka 4.0.0 ë‹¤ìš´ë¡œë“œ ë° ì••ì¶•í•´ì œ
wget https://downloads.apache.org/kafka/4.0.0/kafka_2.13-4.0.0.tgz
tar -xzf kafka_2.13-4.0.0.tgz

# ì™¸ë¶€ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ ë°©í™”ë²½ í•´ì œ
ufw allow 8123
ufw allow 9232 # ê¸°ë³¸ portëŠ” 9000ì„, í˜„ì¬ í™˜ê²½ì—ì„œëŠ” ê²¹ì¹˜ê¸°ì— 9232ë¡œ ë³€ê²½í•´ì„œ ì‚¬ìš©

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì´ˆê¸°í™” (ë©”íƒ€ ë°ì´í„° ì„¤ì •)
bin/kafka-storage.sh format --config config/server.properties --cluster-id $(bin/kafka-storage.sh random-uuid) --standalone

# Kafka Stand-alone êµ¬ë™ í…ŒìŠ¤íŠ¸
# í† í”½ ìƒì„±
/home/kafka/kafka_2.13-4.0.0/bin/kafka-topics.sh --create --topic web-logs-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

# í† í”½ ë¦¬ìŠ¤íŠ¸ í™•ì¸
/home/kafka/kafka_2.13-4.0.0/bin/kafka-topics.sh --list --bootstrap-server localhost:9092

# ë©”ì‹œì§€ Producer
/home/kafka/kafka_2.13-4.0.0/bin/kafka-console-producer.sh --topic web-logs-topic --bootstrap-server localhost:9092

# ë©”ì‹œì§€ Consume
/home/kafka/kafka_2.13-4.0.0/bin/kafka-console-consumer.sh --topic web-logs-topic --from-beginning --bootstrap-server localhost:9092

# ì´ì „ì— ì‘ì„±í•œ FastAPIë¥¼ ì‚¬ìš©í•´ì„œ Middleware Producer ê°œë°œ

# ì´ì „ì— ì‘ì„±í•œ PostgreSQL í…Œì´ë¸” ìƒì„±

# ClickHouse Stand-alone
# Ubuntu ì»¤ë„ ë§¤ê°œë³€ìˆ˜ í™œì„±í™” - ì¬ë¶€íŒ…í•´ë„ ì ìš©ë˜ê²Œ ì„¤ì •
echo 1 | sudo tee /proc/sys/kernel/task_delayacct
sudo sh -c 'echo "kernel.task_delayacct=1" >> /etc/sysctl.conf'
sudo sysctl -p

# ClickHouse Package ì„¤ì • ë° ì„¤ì¹˜ - ì„¤ì¹˜ í•  ë•Œ default ê³„ì • ë¹„ë°€ë²ˆí˜¸ ì…ë ¥í•˜ë¼ê³  ë‚˜ì˜¤ë©´ ì…ë ¥
sudo apt-get install -y apt-transport-https ca-certificates curl gnupg

curl -fsSL 'https://packages.clickhouse.com/rpm/lts/repodata/repomd.xml.key' | sudo gpg --dearmor -o /usr/share/keyrings/clickhouse-keyring.gpg

ARCH=$(dpkg --print-architecture)

echo "deb [signed-by=/usr/share/keyrings/clickhouse-keyring.gpg arch=${ARCH}] https://packages.clickhouse.com/deb stable main" | sudo tee /etc/apt/sources.list.d/clickhouse.list

sudo apt install clickhouse-server clickhouse-client -y

# ClickHouse ì„¤ì • ìˆ˜ì •
sudo vi /etc/clickhouse-server/config.xml

# ì´ë¯¸ ì‚¬ìš©ì¤‘ì¸ portê°€ ìˆì–´ì„œ ì•ˆê²¹ì¹˜ê²Œ ìˆ˜ì •
tcp port 9000 => 9232

# ì™¸ë¶€ì—ì„œë„ ì ‘ì†í•  ìˆ˜ ìˆê²Œ 0.0.0.0ìœ¼ë¡œ ë³€ê²½
<listen_host>0.0.0.0</listen_host>

# ì €ì¥
:w!

# ClickHouse ì‹¤í–‰ ë° ì ‘ì† - ì„¤ì¹˜í•  ë•Œ ì…ë ¥í•œ default ê³„ì •ì˜ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ í•„ìš”
sudo clickhouse start
clickhouse-client --port 9232 # portê°€ ë‹¬ë¼ì„œ ë¶€ì—¬í•œ ê°’

# ClickHouse ì‚¬ìš©ì, DB ìƒì„± ë° ê¶Œí•œ ë¶€ì—¬
create database prd;
create user test IDENTIFIED With plaintext_password by 'password';
GRANT SELECT, INSERT, CREATE, UPDATE, DELETE, TRUNCATE, DROP ON prd.* TO test;

# ì´ì „ì— ì‘ì„±í•œ ClickHouse í…Œì´ë¸” ìƒì„±
```

## Kafka ì‘ì„±

ì´ˆì°½ê¸°ì— ì‘ì„±í•˜ì—¬ ì ìš©í•œ ë²„ì „ì…ë‹ˆë‹¤.

<details>
<summary>ğŸ” <strong>ğŸ” <strong>Kafka Code</strong></summary>

**config.py**

```python
import os
from dotenv import load_dotenv

load_dotenv()

LOG_DIR = os.getenv("LOG_DIR", "./logs")
LOG_FILE = os.path.join(LOG_DIR, "consumer.log")

# PostgreSQL
PG_CONFIG = {
    "host": os.getenv("DB_HOST"),
    "port": os.getenv("DB_PORT"),
    "dbname": os.getenv("DB_NAME"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
}

# ClickHouse
CH_CONFIG = {
    "host": os.getenv("CH_HOST", "localhost"),
    "port": int(os.getenv("CH_PORT", "8123")),
    "username": os.getenv("CH_USER", "default"),
    "password": os.getenv("CH_PASSWORD", ""),
}

# Kafka
KAFKA_CONFIG = {
    "bootstrap.servers": os.getenv("BOOTSTRAP_SERVER"),
    "group.id": os.getenv("GROUP_ID"),
    "auto.offset.reset": os.getenv("OFFSET_RESET_CONFIG"),
    "topic": os.getenv("TOPIC"),
}
```

**ch_client.py**

```python
import clickhouse_connect
from consumer.config import CH_CONFIG
from consumer.logger import logger
from datetime import datetime, timezone, timedelta
import re


def parse_timestamptz(dt_str):
    # ISO 8601 í¬ë§· ë§ì¶°ì„œ ì´ˆ ë‹¨ìœ„ ì´í•˜ ìë¦¿ìˆ˜ ì •ë¦¬ (í•„ìš”ì‹œ)
    # '2025-05-19T13:45:30.123+09:00' ê°™ì€ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜

    # íƒ€ì„ì¡´ ì˜¤í”„ì…‹ í¬í•¨ëœ ë¶€ë¶„ ë¶„ë¦¬
    match = re.match(r"(.*)([+-]\d{2}):(\d{2})$", dt_str)
    if match:
        dt_without_tz = match.group(1)
        tz_hours = int(match.group(2))
        tz_minutes = int(match.group(3))
        tz_delta = timezone(timedelta(hours=tz_hours, minutes=tz_minutes))
        dt = datetime.strptime(dt_without_tz, "%Y-%m-%dT%H:%M:%S.%f")
        dt = dt.replace(tzinfo=tz_delta)
    else:
        # íƒ€ì„ì¡´ì´ Z(UTC)ì¼ ê²½ìš° ì²˜ë¦¬
        dt = datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%S.%fZ")
        dt = dt.replace(tzinfo=timezone.utc)

    # UTCë¡œ ë³€í™˜ í›„ tzinfo ì œê±°
    dt_utc = dt.astimezone(timezone.utc).replace(tzinfo=None)
    return dt_utc


def get_clickhouse_client():
    return clickhouse_connect.get_client(**CH_CONFIG)


def save_to_clickhouse(client, data):
    query = """
    INSERT INTO prd.user_footprint (link, request, footprint_time)
    VALUES (%(link)s, %(request)s, %(footprint_time)s)
    """

    params = {
        "link": data["link"],
        "request": data["method"],
        "footprint_time": parse_timestamptz(data["footprint_time"]),
    }

    client.command(query, params)
    logger.info("ë°ì´í„° ClickHouse ì €ì¥ ì™„ë£Œ")
```

**pg_client.py**

```python
import psycopg2
from consumer.config import PG_CONFIG
from consumer.logger import logger


def get_pg_connection():
    return psycopg2.connect(**PG_CONFIG)


def save_to_postgresql(conn, data):
    with conn.cursor() as cur:
        insert_query = """
        INSERT INTO user_footprint (request, link, footprint_time)
        VALUES (%s, %s, %s)
        """
        cur.execute(
            insert_query, (data["method"], data["link"], data["footprint_time"])
        )
    conn.commit()
    logger.info("ë°ì´í„° PostgreSQL ì €ì¥ ì™„ë£Œ")

```

**main.py**

```python
import json
from confluent_kafka import Consumer, KafkaError
from consumer.logger import logger
from consumer.config import KAFKA_CONFIG
from consumer.pg_client import get_pg_connection, save_to_postgresql
from consumer.ch_client import get_clickhouse_client, save_to_clickhouse


def main():
    consumer = Consumer(
        {
            "bootstrap.servers": KAFKA_CONFIG["bootstrap.servers"],
            "group.id": KAFKA_CONFIG["group.id"],
            "auto.offset.reset": KAFKA_CONFIG["auto.offset.reset"],
        }
    )
    topic = KAFKA_CONFIG["topic"]
    consumer.subscribe([topic])
    logger.info(f"Subscribed to topic: {topic}")

    pg_conn = get_pg_connection()
    ch_client = get_clickhouse_client()
    logger.info("DB ì—°ê²° ì„±ê³µ")

    try:
        while True:
            msg = consumer.poll(1.0)
            if msg is None:
                continue
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    logger.warning(
                        f"End of partition: {msg.topic()} [{msg.partition()}]"
                    )
                else:
                    logger.error(f"Kafka error: {msg.error().str()}")
                continue

            try:
                data = json.loads(msg.value().decode("utf-8"))
                logger.info(f"Received message JSON: {data}")

                save_to_postgresql(pg_conn, data)
                save_to_clickhouse(ch_client, data)

            except json.JSONDecodeError as e:
                logger.error(f"JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}")
            except Exception as e:
                logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")

    except KeyboardInterrupt:
        logger.info("Consumer ì¢…ë£Œë¨")

    finally:
        consumer.close()
        pg_conn.close()
        logger.info("Consumer ë° PostgreSQL ì—°ê²° ì¢…ë£Œ")


if __name__ == "__main__":
    main()
```

</details>
