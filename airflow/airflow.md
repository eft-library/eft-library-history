# ğŸ“‚ ëª©ë¡

- ğŸª¤ [Airflow 3.1.3 êµ¬ì¶•í•˜ê¸°](./airflow.md)
- âš ï¸ [ë°ì´í„° ë¶ˆì¼ì¹˜](./different_data.md)
- ğŸŒ [ë‹¤êµ­ì–´ ë°ì´í„° ë§¤í•‘ ë° ì²˜ë¦¬ëŸ‰ ì¦ê°€ ë¬¸ì œ](./i18n_mapping.md)
- ğŸ”¹ [ë‹¤êµ­ì–´ ì›ì²œ ë°ì´í„°ì˜ ì‹ ë¢°ë„ ë¬¸ì œ](./untranslated_data.md)
- ğŸ“¦ [Data Dump ìë™í™” ì„¤ì •](./data_dump.md)
- ğŸ¹ [ì‹œìŠ¤í…œ Health Check êµ¬ì¶•](./health_check.md)
- ğŸ§  [ì•„ì´í…œ ìƒì„¸ í˜ì´ì§€ ì„±ëŠ¥ íŠœë‹ í›„ê¸°](./item_detail.md)

# ğŸª¤ Airflow 3.1.3 êµ¬ì¶•í•˜ê¸°

AirflowëŠ” 24ì‹œê°„ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ë¯€ë¡œ, í´ë¼ìš°ë“œë¥¼ ì‚¬ìš©í•  ê²½ìš° ì§€ì†ì ì¸ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ ê³ ë ¤í•˜ì—¬ **ì˜¨í”„ë ˆë¯¸ìŠ¤ í™˜ê²½**ì—ì„œ ìš´ì˜í•˜ëŠ” ê²ƒì´ ë¹„ìš© ë©´ì—ì„œ ë” ìœ ë¦¬í•˜ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.

# Rocky Linux 10.1 ë²„ì „ì— Airflow ì„¤ì¹˜í•˜ê¸°

ê³µì‹ Docker Imageë¥¼ í™œìš©í•´, í•„ìš”í•œ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤.

## Git clone

ê¸°ì¡´ì— ì œê°€ ì‘ì„±í•œ ì½”ë“œë“¤ì„ ë‚´ë ¤ ë°›ìŠµë‹ˆë‹¤.

```shell
# Git clone
mkdir /home/airflow
git clone <airflow_project_link>

# í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¶Œí•œ, ì†Œìœ ì ë“±ë¡
mkdir latest_data config tmp logs
chmod 777 -R /home/airflow
chwon 50000:50000 -R /home/airflow
```

> **ì €ëŠ” ê·€ì°®ì•„ì„œ 777ë¡œ ë‹¤ ì£¼ì—ˆëŠ”ë° ì´ëŸ¬ì‹œë©´ ì•ˆë©ë‹ˆë‹¤...**

# Airflow 3.1.3 ì„¤ì¹˜

Docker Imageë¥¼ Pull í•´ì¤ë‹ˆë‹¤. - í•´ë‹¹ ë²„ì „ì€ Airflow 3.1.3 ì…ë‹ˆë‹¤.

ê·¸ë¦¬ê³  ê³µì‹ë¬¸ì„œì—ì„œ docker-compose.yamlì„ ë°›ì•„, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.     
[Docker YML 3.1.3 Link](https://airflow.apache.org/docs/apache-airflow/3.1.3/docker-compose.yaml)

```shell
# Image pull
docker pull apache/airflow:latest-python3.13
```

## docker-compose.yml ì‘ì„±

ì €ëŠ” LocalExecutorë¥¼ ì‚¬ìš©í•˜ê³ , metaë¥¼ ì €ì¥í•  DBëŠ” ì™¸ë¶€ì— postgresqlë¡œ ì´ë¯¸ ìˆê¸°ì— ì˜ì¡´ì„±ì„ ì œì™¸í•˜ê³  ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.

> AIRFLOW__API_AUTH__JWT_SECRET: **ì´ê±° ê¼­ ë§Œë“¤ì–´ì¤˜ì•¼ í•©ë‹ˆë‹¤. ì•„ë‹ˆë©´ LocalExecutorì—ì„œ state mismatch ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.**

```yml
x-airflow-common:
  &airflow-common
  image: apache/airflow:latest-python3.13
  build: ./docker-airflow-fab
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://tkl:TKL%402635%21@1.1.1.1:12345/airflow_meta
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__API_AUTH__JWT_SECRET: 'pTirJtP+QjqHB0YgUcNNaCPtUdEmIrOjNt7QnQXX8XE='
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
    - /home/airflow/eft-library-airflow/dags:/opt/airflow/dags
    - /home/airflow/eft-library-airflow/plugins:/opt/airflow/plugins
    - /home/airflow/logs:/opt/airflow/logs
    - /home/airflow/tmp:/opt/airflow/tmp
    - /home/airflow/latest_data:/opt/airflow/latest_data
  user: "${AIRFLOW_UID:-50000}:0"

services:
  airflow-apiserver:
    <<: *airflow-common
    command: api-server
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
          export AIRFLOW_UID=$$(id -u)
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        echo
        echo "Creating missing opt dirs if missing:"
        echo
        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Airflow version:"
        /entrypoint airflow version
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Running airflow config list to create default config file if missing."
        echo
        /entrypoint airflow config list >/dev/null
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
        echo
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        echo
        echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
        echo
        chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}

    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: 'admin'
      _AIRFLOW_WWW_USER_PASSWORD: 'password'
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
```

## DockerFile ìƒì„±

Airflow 3.X ë²„ì „ ë¶€í„°ëŠ” Fabê°€ ì™„ì „ ì œê±° ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ë¡œê·¸ì¸ ë°©ì‹ì´ì—ˆëŠ”ë° ë³´ì•ˆì„ ìš°ë ¤í•´ì„œì¸ì§€ ì™„ì „ ë¹¼ë²„ë ¸ìŠµë‹ˆë‹¤.      
ì´ë¥¼ ëŒ€ì‹ í•˜ëŠ” SimpleManagerë¥¼ ì‚¬ìš©í•´ì„œ Jsonìœ¼ë¡œ ê³„ì •ê³¼ ë¹„ë°€ë²ˆí˜¸ë¥¼ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì´ ìˆëŠ”ë°, Fab ì“°ëŠ”ê²Œ ë” í¸í•´ì„œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.  

> SimpleManagerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” ì•„ì´ë””ì™€, ë¹„ë°€ë²ˆí˜¸ë¥¼ ì§ì ‘ ë§Œë“¤ ìˆ˜ ì—†ê³  api-server ì—ì„œ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

```shell
# í´ë” ìƒì„± ë° ì´ë™
mkdir docker-airflow-fab
cd docker-airflow-fab

# ì‘ì„± ë“¤ì–´ê°€ê¸°
vi Dockerfile

# ì—¬ê¸°ì„œ ë¶€í„°ëŠ” ë‚´ìš© ì…ë ¥
# ê¸°ì¡´ Airflow ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©
FROM apache/airflow:latest-python3.13

# FAB provider ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜ - ì—†ìœ¼ë©´ ì—ëŸ¬ ë‚¨
RUN pip install --no-cache-dir --user apache-airflow-providers-fab flask_appbuilder flask-session "connexion<3"

# ë‹¤ì‹œ airflow ìœ ì €ë¡œ ë³€ê²½ (ê¶Œì¥)
USER ${AIRFLOW_UID:-50000}
```

## Docker Compose UP

ì‹¤í–‰í•˜ë©´ì„œ ì—ëŸ¬ëŠ” ì—†ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

```shell
docker compose up â€”build
```

## PG Dump ì„¤ì •í•˜ê¸° (.pgpass)

PostgreSQL ë°ì´í„°ë¥¼ Dump ëœ¨ê¸° ìœ„í•´ Airflow Dagë¥¼ ë§Œë“¤ì–´ë„ ê¶Œí•œ ë¬¸ì œì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ë¬»ëŠ” ê²ƒì´ ë‚˜ì™€ì„œ ë§‰íˆê²Œ ë©ë‹ˆë‹¤.

Airflow Dagë¥¼ ì‹¤í–‰í•˜ëŠ” ì£¼ì²´ëŠ” Dockerì˜ Containerì´ë‹ˆ ë‚´ë¶€ì—ì„œ .pgpassë¥¼ ì„¤ì •í•˜ì—¬ ë¹„ë°€ë²ˆí˜¸ë¥¼ ë¬»ì§€ ì•Šê²Œ ì„¤ì •í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.

```shell
# scheduler container ì ‘ì†
docker exec -it --user airflow <scheduler container id> /bin/bash

# pgpass ì‘ì„±
cd /home/airflow
echo "192.168.219.102:dbport:db:user:password" >> ~/.pgpass
chmod 600 /home/airflow/.pgpass
chown airflow:root .pgpass

# test
pg_dump -h 192.168.219.102 -p dbport -U user --inserts db > /opt/airflow/latest_data/test_backup.sql
```

# UIì—ì„œ ì»¤ë„¥ì…˜ ì„¤ì •í•˜ê¸°

DBì™€ Mail ì»¤ë„¥ì…˜ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.

## DB Connection

ì•„ë˜ ì‚¬ì§„ì— ì •ë³´ë¥¼ ì…ë ¥í•˜ë©´ ë©ë‹ˆë‹¤.

## Gmail Connection

ì €ëŠ” ë©”ì¼ì˜ ê²½ìš° Gmailì„ ì‚¬ìš©í–ˆëŠ”ë°, ë‹¤ìŒê³¼ ê°™ì´ í•˜ë©´ ë©ë‹ˆë‹¤.

ë¹„ë°€ë²ˆí˜¸ëŠ” [Gmail IMAP ìƒì„±](https://milkyspace.tistory.com/131) ì—¬ê¸°ë¥¼ ë³´ê³  ë”°ë¼í•˜ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œ ë©ë‹ˆë‹¤.
